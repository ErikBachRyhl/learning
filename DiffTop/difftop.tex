\documentclass[11pt]{article}
\input{../preamble.tex}   % path is relative to the topic folder

\title{Math 214 - Differential Topology}
\author{Erik Bach Ryhl}
\date{\today}

\graphicspath{{./figures}}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Tangent Vectors and Differentials}
\subsection{Overview}
\begin{itemize}
    \item \textgreen{Tangent vectors}
    \item \textgreen{Tangent space}
    \item \textgreen{Smooth maps between manifolds induce maps between tangent spaces: the differential} 
    \item \textgreen{Tangent vectors in local coordinates} 
    \item \textgreen{Tangent space basis: All tangent vectors at \(p\) can be written as a linear combination of these. } 
    \item \textgreen{The coordinates of a tangent vector depends on all the coordinate maps chosen. Example with the change of coordinates rule} 
    \item \textgreen{Review of differentiability in Euclidean space} 
    \item \textgreen{Review of chain rule in Euclidean space} 
\end{itemize}

\subsection{Problems}
\textbf{From HW} 
\begin{itemize}
    \item
\end{itemize}

\textbf{Extra} 
\begin{itemize}
    \item Lee Problem 3.2
\end{itemize}

\subsection{Tangent Vectors as Derivations}
Defined as \textbf{derivations at a point \(p \in M\)}, i.e. \(\mathbb{R}\)-linear maps satisfying the Leibniz Rule. Equipped with a natural vector space structure, we get the vector space of all derivations at \(p\) - this is the \textbf{tangent space \(T_p M\)}. 

Just like a derivative, tangent vectors send constant functions to zero. Also, if two functions \(f, g \in C^\infty(M)\) vanish at the same point \(p \in M\), then \(v_p(fg) = 0\) by the Leibniz rule. This actually recovers an important property of derivatives (which we are trying to generalize). For if we take any function \(C ^\infty(M)\) and put local coordinates \(\varphi = (x^1, \dots , x^n)\) around \(p \in M\), then we can Taylor-expand the coordinate representation of \(f\) around the coordinate representation of \(p\): 

Let \(\hat{f} \coloneqq f \circ \varphi ^{-1}  : \hat{U} \subseteq \mathbb{R} ^n \to \mathbb{R}\), let \(\hat{p} = \varphi (p) = (x^1(p), \dots , x^n(p))\) and let \(y \in \hat{U}\). Then \begin{align*}
    \hat{f}(y) = \hat{f}(\hat{p}) + \frac{\partial \hat{f}}{\partial x^i}(\hat{p})(y^i - \hat{p}^i) + (y^i - \hat{p}^i)(y^j - \hat{p}^j) \int _0 ^1 (1 - t) \frac{\partial ^{2} \hat{f}}{\partial x^i \partial x^j} (\hat{p} + t(y - \hat{p})) dt
\end{align*}
Since \(y^i - \hat{p}^i : \mathbb{R} ^n \to \mathbb{R}\), are a smooth maps between manifolds vanishing at the point \(\hat{p}\), any tangent vector \(v \in T_{\hat{p}} \mathbb{R} ^n\) will annihilate their product due to the Leibniz rule. Similarly, \(\hat{f}(\hat{p})\) is a constant. So the only term that remains is \begin{align}\label{eq:tangent_vector_coordinate_rep_action}
    v(\hat{f}(\hat{p})) = \frac{\partial \hat{f}}{\partial x^i}(\hat{p})\left( v(y^i) - v(\hat{p}^i) \right)  = \frac{\partial \hat{f}}{\partial x^i}(\hat{p})v(y^i) 
\end{align} 
So, the important property of derivatives we recover is that \textbf{the output of acting with a tangent vector on a smooth function will only depend on the functions first order change}. In other words, tangent vectors on manifolds are used for linearly approximating the first order change of smooth functions on manifolds. 

Notice that we could act with a tangent vector \(v \in T_{\hat{p}} \mathbb{R} ^n\) exactly because tangent vectors act \textbf{locally} - they only care about the behavior of our functions arbitrarily close to the specific point we are at on the manifold. Hence if two smooth functions \(f, g \in C^\infty(M)\) agree on some small neighborhood around \(p\), then \(v_p f = v_p g\). In general, for any open subset \(U\) of a manifold (called an \textbf{open submanifold} with subset topology and the obvious atlas constructed from restricting the charts on \(M\)), the differential of the inclusion \(d\iota_p : T_p U \to T_p M\) is an isomorphism between \(T_p U \cong T_p M\). 

\subsection{Differentials}
\textbf{Differentials} are \textbf{linear maps} between tangent spaces of manifolds. Any smooth function \(F: \mathcal{M}\to N\) between two manifolds induces a map between respective tangent spaces through the differential. Given \(p \in M\), \(v \in T_p M\) and \(g \in C^\infty(N)\), we define \(dF_p : T_p \mathcal{M}\to T_{F(p)}N\) as follows: \begin{align*}
    (dF_p v) (g) = v_p (g \circ F)
\end{align*}

\textbf{Properties of differentials:}\begin{itemize}
    \item Differentials are linear maps. Think \((Av)(f) = w(f)\) basically.
    \item Chain rule from calculus abstractly becomes a composition of linear maps: If \(G : N \to P\) is another smooth map, \(d(G \circ F)_p = dG_{F(p)} \circ dF_p : T_p \mathcal{M}\to T_{G(F(p))}N\)
    \item \(d(\operatorname{Id}_\mathcal{M}) = \operatorname{Id}_{T_p M}\)
    \item \textbf{A diffeomorphism \(F : \mathcal{M}\to N\) induces an isomorphism between tangent spaces with \(\left( dF_p \right)^{-1}  = d(F^{-1} )_{F(p)}\)}
\end{itemize} 

We see that the action of an (abstract) tangent vector \(v \in T_p M\) acting on \(f \in C^\infty(M)\) can be "pushed forward" in local coordinates by a chart \(\varphi : U \to  \hat{U} \subseteq \mathbb{R} ^n\): 
\begin{align} \label{eq:tangent_vector_action}
    v_p(f) = v_p(f \circ \varphi ^{-1} \circ \varphi ) = v_p(\hat{f} \circ \varphi ) = (d \varphi _p v_p) (\hat{f}) \textred{\  = v^i \frac{\partial }{\partial x^i} \bigg|_{p} f = v^{i} \frac{\partial \hat{f}}{\partial x^i}  (\hat{p}) } 
\end{align}
The red text is to indicate that we are about to introduce the necessary machinery. But it is already put there to parallel eq. \ref{eq:tangent_vector_coordinate_rep_action} - if \(v^i = v(y^i)\), then they are the same! And this is what we'll see.

\subsubsection{Tangent space of a vector space}
Given a vector space, a vector \(a \in V\) (thought of as a point) and a vector \(v \in V\) (thought of as a direction) gives for every \(f \in C^\infty(V)\) a derivation: 
\begin{align*}
D_v \big|_{a}f = \frac{d}{dt} \bigg|_{t = 0} f(a + tv)     
\end{align*}
The map \(v \mapsto D_v \big|_{a}\) is in fact a canonical isomorphism between \(V\) and \(T_a V\) - hence the tangent space of a vector space can be identified with the vector space itself. And if one has a linear map \(L : V \to W\) between vector spaces, the differential \(dL_a : T_a V \to T_{L(a)}W\) gives a commutative diagram through \begin{align*}
    d L_a (D_v \big|_{a} )(f) = D_{Lv} \big|_{La} f 
\end{align*}   

% anki-tex-guid: a0dd1f364c15
\begin{example}
    Since the vector space \(GL(n, \mathbb{R})\) is an open submanifold of the vector space \(M(n, \mathbb{R})\), we can use both the identification \(T_p GL(n, \mathbb{R} ) \cong T_p M(n, \mathbb{R})\) and the fact that \(M(n, \mathbb{R})\) is a vector space such that \(T_p M(n, \mathbb{R} ) \cong M(n, \mathbb{R})\) to see that \(T_p GL(n, \mathbb{R} ) \cong M(n, \mathbb{R})\). 

    Recall that \(GL(n, \mathbb{R})\) is in fact a Lie group, and as such \(T_e GL(n, \mathbb{R} ) = \mathfrak{g} \cong M(n, \mathbb{R})\). In other words, this canonical construction proves why the Lie algebra of \(GL(n, \mathbb{R})\) is the set of \textit{all} \(n \times n\) matrices over \(\mathbb{R} \).    
\end{example}
\textred{Check up on this after reviewing Lie algebras!}

\subsection{Tangent Vectors in Local Coordinates}
For any \(p \in M\), pick a chart \((U, \varphi) = (U, (x^1, \dots , x^n))\) that contains \(p\). Since \(\varphi : U \to \hat{U} \subseteq \mathbb{R} ^n\) is a diffeomorphism between smooth manifolds (\(U\) is an open submanifold of \(M\) and \(\hat{U}\) is an open submanifold of \(\mathbb{R} ^n\)), its differential is an isomorphism between tangent spaces: \begin{align*}
    d \varphi _p : T_p U \to T_{\varphi (p)} \hat{U}.
\end{align*} 

As mentioned, we can always canonically identify the tangent space of an open submanifold with the tangent space of the ambient manifold. Doing this on both sides, we get that \begin{align*}
    d \varphi _p : T_p \mathcal{M}\to T_{\varphi (p)} \mathbb{R} ^n
\end{align*}
is a canonical isomorphism. Since we already have a basis for \(T_{\varphi (p)} \mathbb{R} ^n\) given by \begin{align*}
    \frac{\partial }{\partial x^1} \bigg|_{\varphi (p)}, \dots ,  \frac{\partial }{\partial x^n} \bigg|_{\varphi (p)},
\end{align*} 
where these are \underline{Euclidean partial derivatives}, we can just "lift these up" to the manifold through the isomorphism \(d \varphi _p\) to obtain a basis for \(T_p M\)! We define them with the notations: \begin{align*}
    \frac{\partial }{\partial x^i} \bigg|_{p} = (d \varphi _p)^{-1} \left( \frac{\partial }{\partial x^i} \bigg|_{\varphi (p)} \right) = d\left(\varphi ^{-1}  _{\varphi (p)}\right)\left( \frac{\partial }{\partial x^i} \bigg|_{\varphi (p)} \right)
\end{align*}
and since it is a basis, \textbf{any (abstract) tangent vector \(T_p M\) can be written as} \begin{align*}
    v_p = v^i \frac{\partial }{\partial x^i} \bigg|_{p}.
\end{align*}
The action of such a tangent vector on a map \(f \in C^\infty(M)\) can then finally be computed explicitly: \begin{align*}
    \frac{\partial}{\partial x^i} \bigg|_{p} f = d\left(\varphi ^{-1}  _{\varphi (p)}\right)\left( \frac{\partial }{\partial x^i} \bigg|_{\varphi (p)} \right) (f) = \frac{\partial }{\partial x^i} \bigg|_{\varphi (p)} \left( f \circ \varphi ^{-1} \right) = \frac{\partial \left( f \circ \varphi ^{-1}  \right) }{\partial x^i} (\varphi (p)) = \frac{\partial \hat{f}}{\partial x^i}(\hat{p})
\end{align*} 
where it should always be noted that "pulling the function up on the fraction" means that we are thinking about Euclidean derivatives. This is just the abstraction we wanted: \textbf{the (abstract) tangent basis vector for the \(i\)'th coordinate function for \(T_p M\) acts in local coordinates by taking the \(i\)'th partial derivative of the coordinate representation of \(f\) at the coordinate representation of the point \(p\)}. A good rule of thumb then, is that one can turn the abstract action of a tangent vector into a Euclidean partial derivative by putting hats on everything and taking the Euclidean partial derivative. 

It is even easy to find the coordinates for a tangent vector \(v_p \in T_p M\), since \textbf{they are just \(v_p\) applied to the coordinate functions}: \begin{align*}
    v_p (x^i) = \left( v^j \frac{\partial }{\partial x^j}\bigg|_{p} \right)  (x^i) &= d \left( \varphi ^{-1} _{\varphi (p)} \right) \left( v^j \frac{\partial }{\partial x^j}\bigg|_{\varphi (p)} \right) (x^i) = v^j\frac{\partial }{\partial x^j}\bigg|_{\varphi (p)} \left( x^i \circ \varphi ^{-1}  \right)\\
    &= v^j \frac{\partial \left( x^i \circ \varphi ^{-1}  \right) }{\partial x^j} (\varphi (p)) = v^j \frac{\partial \left( \pi ^i \circ \varphi \circ \varphi ^{-1}  \right) }{\partial x^j} (\varphi (p)) \\
    &= v^j \frac{\partial \left( \pi ^i \right) }{\partial x^j} (\varphi (p))\\
    &= v^j \delta ^i _j \\
    &= v^i,
\end{align*}
where it was used that by definition, \(x^i = \pi ^i \circ \varphi \) with \(\pi ^i : \mathbb{R} ^n \to \mathbb{R}\) the projection map in Euclidean space. We have now justified \ref{eq:tangent_vector_action}. We can use the same "lift up by the charts" idea to get an expression for the differential in coordinates. 

\subsection{Differentials in Local Coordinates}
Recall the total differential \(DF(a)\), which is defined as the linear map between the vector spaces that \(F: V \to W\) maps between such that \begin{align*}
    \lim_{v \to 0} \frac{|F(a + v) - F(a) - DF(a) v|}{|v|} = 0 
\end{align*}
i.e. it is a linear map which approximates the change of the map \(F\) at \(a \in V\).  

One can prove the (now familiar) chain rule for composition of differentiable maps \begin{align*}
    D(G \circ F)(a) = DG(F(a)) \circ DF(a).
\end{align*}

Introducing the language of partial derivatives, we then find that the matrix representation of the total differential is the Jacobian, i.e. that \begin{align*}
    \left( DF(a) \right) ^i _j = \frac{\partial F^i }{\partial x^j}(a), 
\end{align*}
such that the chain rule for partial derivatives is just matrix multiplication: \begin{align*}
    \left( D(G \circ F)(a) \right) ^i _j = \left( DG(F(a)) \circ DF(a) \right) ^i _j = \left( DG(F(a)) \right)^i _k \left( DF(a) \right) ^k _j = \frac{\partial G^i}{\partial y^k}(F(a)) \frac{\partial F^k}{\partial x^j} (a)  
\end{align*}

These are exactly the things we are trying to generalize to our manifolds. 

\paragraph{Euclidean Version:}
First, let us look at a Euclidean map \(F: U \subseteq \mathbb{R} ^n \to V \subseteq \mathbb{R} ^m\), but thinking of Euclidean space as a manifold now. Let \(p \in U\) with local coordinates \((x^1, \dots , x^n)\) and let \((y^1, \dots , y^m)\) be local coordinates for \(V\). Let us take some tangent vector in \(T_{p} U \cong T_{p}\mathbb{R} ^n\). Then we can push it forward with the differential
\begin{align*}
    d F_{p} \left( \frac{\partial }{\partial x^i} \bigg|_{p}  \right) f = \frac{\partial }{\partial x^i}\bigg|_{p} \left( f \circ F \right) = \frac{\partial f}{\partial y^j}(F(p)) \frac{\partial F^j}{\partial x^i}(p) = \left( \frac{\partial F^j}{\partial x^i}(p) \frac{\partial }{\partial y^j} \bigg|_{F(p)}   \right) f
\end{align*}  
such that \begin{align*}
    dF_p \left( \frac{\partial }{\partial x^i} \bigg|_{p}  \right) =\frac{\partial F^j}{\partial x^i}(p) \frac{\partial }{\partial y^j} \bigg|_{F(p)}.
\end{align*}
Note that this is all Euclidean and is exactly what we would expect. Parallel this with conventions for representing linear maps in linear algebra: Given a linear map \(L: V \to W\) between spaces with bases \(\left\{ e_i \right\}_{i = 1}^n \) and \(\left\{ f_j \right\}_{j = 1}^m \) respectively, we by convention write \begin{align*}
    Le_i = A^j_i f_j.
\end{align*}
Now, given any vector \(v \in V\) represented by \begin{align*}
    v = v^i e_i
\end{align*} 
we would find that \begin{align*}
    Lv = L(v ^i e_i) = v^i Le_i = v^i A^j_i f_j = (A ^j _i v^i )f_j = (Av)^j f_j.
\end{align*}
So the coordinate vector of \(v\) is sent to the coordinate vector \(Av\). In other words, the representation of the linear map acts with matrix multiplication on the coordinate representation of the vector. This directly lets us see that the \(i\)'th column of \(A\) is just the coordinate vector of \(L(e_i)\). So remember that \textbf{the \(i\)'th column of \(A\) is the image of the \(i\)'th basis vector of the domain of the map that \(A\) represents.}

Replacing \(L \leftrightarrow dF_p\) and \(e_i \leftrightarrow \frac{\partial }{\partial x^i} \big|_p\) and \(f_j \leftrightarrow \frac{\partial }{\partial y^j} \big|_{F(p)}\), we can directly interpret \begin{align*}
    \frac{\partial F^j}{\partial x^i} (p)
\end{align*}  
as the \(j\)'th row and \(i\)'th column of the Jacobian of \(DF(p)\) and hence that column \(j\) in the Jacobian matrix is the coordinate vector of the basis vector \(\frac{\partial }{\partial x^i} \big|_p \in T_p \mathbb{R} ^n\) expressed in the basis \(\frac{\partial }{\partial y^j}\big|_{F(p)} \in T_{F(p)} \mathbb{R} ^m\). 

Let us generalize to manifolds. Given a smooth map \(F: \mathcal{M}\to N\), choose smooth coordinate charts \((U, \varphi ) = (U, x^1, \dots , x^m)\) for \(M\) containing \(p\) and \((V, \psi) = (V, y^1, \dots , y^n)\) for \(N\) containing \(F(p)\). We can go through the same steps above but just with the coordinate representation \(\hat{F} = \psi \circ F \circ \varphi ^{-1} : \varphi (U \cap F ^{-1} (V)) \to \psi (V)\) and we would get that \(d \hat{F}_{\hat{p}} \) is represented by the Jacobian matrix of the coordinate reprensetative for \(F\) at the coordinate representative for \(p\). But we want to figure out what it is directly for \(dF_p\): \begin{align*}
    dF_p \left( \frac{\partial}{\partial x^i}\bigg|_{p} \right) &= dF_p \left( d(\varphi ^{-1} )_{\hat{p}} \left( \frac{\partial}{\partial x^i}\bigg|_{\hat{p}}\right)\right) = d(\psi ^{-1} )_{\psi(F(\varphi ^{-1} (x)))} d \hat{F}_{\hat{p}} \left( \frac{\partial}{\partial x^i}\bigg|_{\hat{p}}  \right) \\
    &= d(\psi ^{-1} )_{\hat{F}(\hat{p})} \left( \frac{\partial \hat{F}^j}{\partial x^i}(\hat{p}) \frac{\partial }{\partial y^j} \bigg|_{\hat{F}(\hat{p})}\right) \\
    &= \frac{\partial \hat{F}^j}{\partial x^i}(\hat{p}) \left[  d(\psi ^{-1} )_{\hat{F}(\hat{p})} \left( \frac{\partial}{\partial y^j}\bigg|_{\hat{F}(\hat{p})}\right)  \right] \\
    &= \frac{\partial \hat{F}^j}{\partial x^i}(\hat{p}) \frac{\partial }{\partial y^j}\bigg|_{F(p)}
\end{align*}  
\begin{align*}
    \implies \boxed{dF_p \left( \frac{\partial}{\partial x^i}\bigg|_{p}  \right) = \frac{\partial \hat{F}^j}{\partial x^i}(\hat{p}) \frac{\partial }{\partial y^j}\bigg|_{F(p)}}
\end{align*}
Hence \(dF_p\) is represented in a coordinate basis by the Jacobian matrix of the coordinate representation of \(F\). This was the ultimate goal of our abstraction of derivatives: to have a way to talk about derivatives between manifolds, such that it reproduces the Euclidean version once we are in local coordinates, but which is defined purely in terms of abstract actions on maps on between manifolds.

\subsection{Change of coordinates}

Let \((U, \varphi = (x^1, \dots , x^n))\) and \((V, \psi = (\tilde{x}^1, \dots , \tilde{x}^n))\) be charts for \(M\) and \(p \in U \cap V\). If \(x \in \hat{U}\subseteq \mathbb{R} ^n\), one can think of the representation of the transition map as \begin{align*}
    \psi \circ \varphi ^{-1} (x) = (\tilde{x}^1(x), \dots, \tilde{x}^n(x)),
\end{align*}
i.e. that \textbf{the new first coordinate is a smooth function of \textit{all} the other old coordinates} etc. Note the emphasis on "all" here. This is of course a slight abuse of notation since we can't directly apply the coordinate function \(\tilde{x}^i : \mathcal{M}\to \hat{V}\subseteq \mathbb{R} ^n\) to a point in \(\mathbb{R} ^n\), but as long as we remember that the transition maps "factors through" the manifold, it is a good way to think about it.

Just like chart maps are diffeomorphisms, so are their compositions. In other words, transition maps are diffeomorphisms, and hence their differentials are isomorphisms. With the general formula for the coordinate representation of the differential action, we find that \begin{align*}
    d(\psi \circ \varphi ^{-1} )_{\varphi (p)} \left( \frac{\partial}{\partial x^i}\bigg|_{\varphi (p)}  \right) = \frac{\partial (\psi \circ \varphi ^{-1} )^j}{\partial x^i} (\varphi (p)) \frac{\partial}{\partial \tilde{x}^j}\bigg|_{\psi (p)} = \frac{\partial \tilde{x}^j}{\partial x^i}(\varphi (p))\frac{\partial}{\partial \tilde{x}^j}\bigg|_{\psi (p)}  ,
\end{align*}

% &= d(\varphi ^{-1} )_{\varphi (p)}\left( \frac{\partial}{\partial x^i}\bigg|_{\varphi (p)}  \right) = d(\psi ^{-1} \circ \psi \circ \varphi ^{-1} )_{\varphi (p)} \left( \frac{\partial}{\partial x^i}\bigg|_{\varphi (p)} \right)  \\
% &= d (\psi ^{-1} )_{\psi (p)} \frac{\partial \tilde{x}^j}{\partial x^i}(\varphi (p))\frac{\partial}{\partial \tilde{x}^j}\bigg|_{\psi (p)}

such that \begin{align*}
    \boxed{\frac{\partial}{\partial x^i}\bigg|_{p} = \frac{\partial \tilde{x}^j}{\partial x^i}(\varphi (p))\frac{\partial}{\partial \tilde{x}^j}\bigg|_{p} = \frac{\partial \tilde{x}^j}{\partial x^i} (x(p)) \frac{\partial}{\partial \tilde{x}^j}\bigg|_{p}}
\end{align*}
where the latter notation used is \(\varphi (p) = (x^1(p), \dots , x^n(p)) \coloneqq x(p)\), which I think makes it a better mnemonic to remember which of the chart representations one is supposed to input.  

Hence for a tangent vector \(v \in T_p M\), which can be written as \(v = v^i \partial / \partial x^i \big|_{p} = \tilde{v}^j \partial / \partial \tilde{x}^j \big|_{p}\) we get \begin{align*}
    \boxed{\tilde{v}^j = \frac{\partial \tilde{x}^j}{\partial x^i}(\varphi (p))  v^i = \frac{\partial \tilde{x}^j}{\partial x^i}(x(p)) v^i }
\end{align*} 

Remember that abstract tangent vectors \(\partial / \partial x^i \big|_{p} \) has the action of taking the \(i\)-th partial derivative of the coordinate representation of a function. Partial derivatives keep all other coordinates constant, so if one changes any coordinate function \(x^j\) of the chart \(\varphi = (x^1, \dots , x^n)\), then "keep all other coordinates constant" means something different. Hence just like in linear algebra, the coordinate representaion depends on the entire basis that one has chosen.    

% anki-tex-guid: 9068656070f6
\begin{example}
    Checking that a map / definition is coordinate independent. See the example that Dr. Freud posted for HW II in the chat.
\end{example}

\subsection{Tangent Vectors as Curves}
Let \(J \subseteq \mathbb{R}\) be some interval. If we want to be very formal, we can denote our smooth chart as \((J, t)\). \(\mathbb{R} \) being one-dimensional gives us a single tangent vector as our basis for any point \(t \in J\), namely \begin{align*}
    \frac{d}{dt}\bigg|_{t_0},
\end{align*} 
where \(t_0 \in J\) is some fixed point we've chosen.

Let us consider \(f \in C^\infty(M)\). Given a smooth curve \(\gamma : J \subseteq \mathbb{R} \to M\), we can use the differential to push our tangent vector forwards to \(M\), since \(d \gamma _{t_0}: \mathbb{R} \to T_{\gamma (t_0)}M\): \begin{align*}
    d \gamma _{t_0}\left( \frac{d}{dt} \bigg|_{t_0} \right)f = \frac{d}{dt} \bigg|_{t_0} (f \circ \gamma) = (f \circ \gamma )^{\prime} (t_0)
\end{align*}
Since the domain of \(d \gamma_{t_0} \) is already Euclidean space, we could use the chain rule without further ado. We define \begin{align*}
    \gamma ^{\prime} (t_0) f \coloneqq d \gamma _{t_0} \left( \frac{d}{dt}\bigg|_{t_0} \right) f = \frac{d}{dt}\bigg|_{t_0} (f \circ \gamma )
\end{align*}

If \((U, \varphi = (x^1, \dots , x^n))\) are charts around \(p = \gamma (t_0)\), we'll write \(\gamma (t) = (x^1(\gamma (t)), \dots , x^n(\gamma (t))) \coloneqq (\gamma ^1(t), \dots , \gamma ^n (t))\). Then using our coordinate representation formula for \(dF_p (v)\) we find that \begin{align*}
    \boxed{\gamma ^{\prime} (t_0) = \frac{d \gamma ^i}{d t} (t_0) \frac{\partial }{\partial x^i} \bigg|_{\gamma (t_0)}}
\end{align*}
Every tangent vector \(v \in T_p M\) is the velocity vector of some curve. By picking the curve \(\gamma (t) = \varphi ^{-1} (t v^1, \dots, t v^n) \coloneqq (t v^1, \dots , tv^{n} )\) and a chart on \(M\) centered at \(p \in M\), we see that \begin{align*}
    \gamma ^{\prime} (0) = \frac{d \hat{\gamma}^i}{dt} (0) \frac{\partial }{\partial x^i}\bigg|_{\gamma (0)} = v^i \frac{\partial }{\partial x^i} \bigg|_{p} = v  
\end{align*} 

In fact, for any composite maps between curves and manifold maps we define with the prime notation as follows. Take \(F : \mathcal{M}\to N\) and \(\gamma (t)\) as before. Then \begin{align*}
    (F \circ \gamma )^{\prime} (t_0) \coloneqq d(F \circ \gamma )_{t_0} \left( \frac{d}{dt}\bigg|_{t_0} \right) = dF_{\gamma (t_0)} \circ d \gamma _{t_0} \left( \frac{d}{dt}\bigg|_{t_0} \right) = dF _{\gamma (t_0)} (\gamma ^{\prime} (t_0)).
\end{align*} 
This also makes it clear that if we take a curve \(\gamma : J \to M\) with \(\gamma (t_0) = p\) and \(\gamma ^{\prime} (t_0) = v\), then \begin{align*}
    dF_p (v) = dF_{\gamma (t_0)} (\gamma ^{\prime} (t_0)) \coloneqq (F \circ \gamma )^{\prime} (t_0)
\end{align*}
Often we choose a chart centered at \(p\) and choose \(t_0 = 0\). 

\subsection{The Tangent Bundle}

The tangent bundle is defined as the disjoint union of all the tangent spaces of \(M\): \begin{align*}
    T\mathcal{M}= \coprod _{p \in M} T_p \mathcal{M}
\end{align*}

Every element is written as \((p, v) \in TM\) and we have the natural projection map \(\pi : T\mathcal{M}\to M\) sending \((p, v) \mapsto p\). There is no canonical way to project onto the second factor, since each tangent space is formally disjoint to all other tangent spaces. In other words, there is no canonical way to determine which vector one should transition to when transitioning between tangent spaces. The only canonical vector in any vector space is the zero vector, so when the zero vector is involved one can sometimes make canonical identifications. Just like with the zero section \(Z\) for example.

For any \(v \in T_p M\), we can identify it with \((p, v) \in TM\) canonically, since the injection of \(p\) is canonical.  

\subsection{The manifold structure of \(TM\)}
One can think of \(TM\) as locally attaching to each point of \(p \in M\) the entire tangent space \(T_p M\). The word "locally" is very important here, since one cannot always globally write the tangent bundle as a cartesian product. But locally, that intuition is good.

The open chart domains on \(TM\) will be made from open chat domains \(U \subseteq M\) through the preimage of the projection map. First of all, note that for any \(p \in M\), \begin{align*}
    \pi ^{-1} (p) = T_p M,
\end{align*} 
where \(\pi ^{-1} (p)\) is called \textbf{the fiber over \(p\)}. Then \begin{align*}
    \pi ^{-1} (U) = \pi ^{-1} \left( \bigcup_{p \in U} p \right) = \bigcup_{p \in U} \pi ^{-1} (p) = \bigcup_{p \in U} T_p M,
\end{align*} 
which means that \(\pi ^{-1} (U)\) is the union of all the tangent vectors to all points in \(U\), or equivalently, the union of all the fibers under \(\pi\) over points in \(U\). The charts are defined as pairs \((\pi ^{-1} (U), \tilde{\varphi })\), with \(\tilde{\varphi }: TM \to \varphi (U) \times \mathbb{R} ^n \subseteq \mathbb{R} ^{2n}\) as \begin{align*}
    \tilde{\varphi } \left( v^i \frac{\partial}{\partial x^i}\bigg|_{p}  \right) = (\varphi (p), v_p) = (x^1 (p), \dots , x^n(p), v^1, \dots , v^n).
\end{align*} 
It is a bijection, since \begin{align*}
    \tilde{\varphi}^{-1} (x^1(p), \dots , x^n(p), v^1, \dots , v^n) \coloneqq v^i \frac{\partial}{\partial x^i}\bigg|_{\varphi ^{-1} (x)} 
\end{align*}
is an explicit inverse. 

Note that to use \(\tilde{\varphi}\), where one "strips" the coordinates of tangent vector from the basis vectors, the tangent vector needs to be written in the basis related to the charts specified by \(\varphi (p)\). To illustrate what I mean, consider \((\pi ^{-1} (U), \tilde{\varphi})\) and \((\pi ^{-1} (V), \tilde{\psi })\) with \((p, v) \in \pi ^{-1} (U) \cap \pi ^{-1} (V) = \pi ^{-1} (U \cap V)\). Then the transition map \(\tilde{\psi } \circ \tilde{\varphi}^{-1} \) takes a little bit of computation. Let \((x^1, \dots , x^n, v^1, \dots , v^n) \in \mathbb{R} ^{2n}\): \begin{align*}
    \tilde{\psi }\circ \tilde{\varphi }^{-1} (x^1, \dots , x^n, v^1, \dots , v^n) &= \tilde{\psi } \left( v^i \frac{\partial}{\partial x^i}\bigg|_{\varphi ^{-1} (x)}  \right) \\
    &= \tilde{\psi} \left( v^i\frac{\partial (\psi \circ \varphi ^{-1} )^j}{\partial x^i} (x) \frac{\partial}{\partial \tilde{x}^j}\bigg|_{\varphi ^{-1} (x)}  \right) \\
    &= \tilde{\psi} \left( v^i\frac{\partial \tilde{x}^j}{\partial x^i} (x) \frac{\partial}{\partial \tilde{x}^j}\bigg|_{\psi ^{-1} (\tilde{x})}  \right) \\
    &= \left( \tilde{x}^1(x), \dots , \tilde{x}^n(x), \frac{\partial \tilde{x}^1}{\partial x^i} (x) v^i, \dots , \frac{\partial \tilde{x}^n}{\partial x^i} (x)  v^i \right) 
\end{align*}

Note that since \(\varphi^{-1} (x) \in U \cap V\), we can write it as either \(\psi ^{-1} (x)\) or \(\varphi ^{-1} (\tilde{x})\). To remember the above a bit more easily, just note the linear algebra analogue: \begin{align*}
    \tilde{\psi } \circ \tilde{\varphi }^{-1} (x, v) = (\tilde{x}(x), J(x) v),
\end{align*}  
where \begin{align*}
    J(x) = \frac{\partial \tilde{x}^j}{\partial x^i}(x) 
\end{align*}
is just the Jacobian of the transition map. The reason we need all the transition map differentiation is because it would have been completely wrong to write \begin{align*}
    \tilde{\psi } \left( v^i \frac{\partial}{\partial x^i}\bigg|_{\psi ^{-1} (x)}  \right)  = \left( \tilde{x}^1 (x), \tilde{x}^n(x), v^1, \dots , v^n \right),
\end{align*}
since if we would then be implictly equating \begin{align*}
    \textred{v = v^i \frac{\partial}{\partial \tilde{x}^i}\bigg|_{\psi ^{-1} (x)}} 
\end{align*}
i.e. combining coordinates in one bases with basis vectors in another. The transition map above is smooth and with a few more arguments we see that the construction gives a smooth structure for \(TM\). This immediately implies that \(\tilde{\varphi} : \pi ^{-1} (U) \to \varphi (U) \times \mathbb{R} ^n\) is a diffeomorphism. 

Hence if a manifold \(M\) can be covered by a single chart, then \(\varphi (M) = \hat{U}\subseteq \mathbb{R} ^n\) is a diffeomorphism, and thus \(T\mathcal{M}\cong \mathcal{M}\mathbb{R} ^n\) are diffeomorphic. So in cases like this, the tangent bundle \textit{can} be considered as a cartesian product \textit{globally}.

\textred{Manifold chart lemma is used to finish the proof that \(TM\) is a \(2n\) dimensional smooth manifold}

% anki-tex-guid: 95fd6210abdb
\begin{example}
    \(TS^1 \cong S^1 \times R\).   
\end{example}

\subsection{The Global Differential} 
Every smooth map \(F : \mathcal{M}\to N\) induces a smooth map \(dF : T\mathcal{M}\to TN\) called the global differential. It is the map whose restriction to each tangent space \(T_p \mathcal{M}\subseteq TM\) is \(dF_p\). Hence it's action on \(v \in TM\) is \begin{align*}
    dF (v) = dF_p (v)
\end{align*}

Writing the map in local coordinates we see that it is smooth, since
\begin{align*}
    \tilde{dF} (x^1, \dots , x^n, v^1, \dots , v^n) &= \tilde{\psi } \circ dF \circ \tilde{\varphi}^{-1} (x^1, \dots , x^n, v^1, \dots , v^n) \\
    &= \tilde{\psi } \circ dF \left( v^i \frac{\partial}{\partial x^i}\bigg|_{\varphi ^{-1} (x)}  \right) \\
    &= \tilde{\psi } \left( dF_{\varphi ^{-1} (x)} \left( v^i \frac{\partial}{\partial x^i}\bigg|_{\varphi ^{-1} (x)}  \right) \right)  \\
    &= \tilde{\psi } \left( v^i \frac{\partial \hat{F}^j}{\partial x^i} (\varphi (\varphi ^{-1} (x)) ) \frac{\partial}{\partial \tilde{x}^j}\bigg|_{\varphi ^{-1} (x)}  \right) \\
    &= \left( \psi^1(F(\varphi ^{-1} (x))), \dots , \psi ^n (F(\varphi ^{-1} (x))),\frac{\partial \hat{F}^1}{\partial x^i} (x) v^i, \dots , \frac{\partial \hat{F}^n}{\partial x^i} (x) v^i \right) \\
    &= \left( \hat{F}^1(x), \dots , \hat{F}^n(x), \frac{\partial \hat{F}^1}{\partial x^i} (x) v^i, \dots , \frac{\partial \hat{F}^n}{\partial x^i} (x) v^i  \right), 
\end{align*}
which is smooth because \(F\) is. Of course global differentials has exactly the same nice properties that local ones have. Note also that differentials "respect" base-points: \(\pi (dF(v_p)) = F(p)\), since \(dF(v_p) = dF_p(v_p) \in T_{F(p)}N\).   


\subsection{The cotangent space}

\subsection{Differentials of \(C^\infty(M)\) maps as covectors and a covector basis}


\section{Lie Groups}

\section{Vector Bundles and Differential Forms}
\paragraph{Abstract}
\newpage\lecture{17}{Thu 23 Oct}{Introduction to Vector Bundles}
% anki-tex-guid: 7ec3984b0717
\begin{definition}[Vector Bundle]
    A (rank \(k\), real, smooth) \textbf{vector bundle} over \(M\) is a smooth manifold \(E\) of dimension \(n + k\), and a smooth map \(\pi : E \to M\) such that \begin{itemize}
        \item For each \(p \in M\), the \textbf{fiber} \(\pi ^{-1} (p)\) has the structure of a (real \(k\)-dimensional) vector space, and
        \item (\textbf{Local Triviality}) For each \(p \in M\), there is a nbhd \(U\) of \(p\) and a diffeomorphism \(\phi : \pi ^{-1} (U) \to U \times \mathbb{R} ^k\) such that for each \(p \in U\), the restriction \begin{align*}
            \phi \big|_{\pi ^{-1} (p)}: \pi ^{-1} (p) \to  \left\{ p \right\} \times \mathbb{R} ^k 
        \end{align*} is an isomorphism of vector spaces. 
    \end{itemize}  
    \[
    \begin{tikzcd}
    \pi ^{-1} (U) \arrow[r, "\phi", "\sim"'] \arrow[d, "\pi"'] & U \times \mathbb{R} ^k \arrow[d, "\pi _1"] \\
    U \arrow[r, "\operatorname{id} "']                & U
    \end{tikzcd}
    \]

    A vector bundle is thus a triplet, often just written as \begin{align*}
        E \overset{\pi }{\longrightarrow} M
    \end{align*}
    In the context of a vector bundle, \(E\) is called the \textbf{total space}, \(M\) is called the \textbf{base space} and \(\pi\) is called the \textbf{projection}.    
\end{definition}

So just like the tangent bundle, we think of attaching a vector space to each point of \(M\) like a piece of string (a fiber is vizualised this way, hence the name). But vector bundles are more general than the tangent bundle construction. I think that they were inspired by the tangent bundle perhaps. 

\begin{note}
    Write some more intuition for it here
\end{note} 

% anki-tex-guid: b9cfcab32483
\begin{example}[Trivial Vector Bundle]
    The trivial vector bundle has \(E = \mathcal{M}\times \mathbb{R} ^k\), \(\pi = \pi _1 : \mathcal{M}\times \mathbb{R} ^k \to M\). 
\end{example}

\begin{remark}
    It follows from the definition that \(\pi\) is a submersion and that the vector space operations on the fibers \(\pi ^{-1} (p)\) "depend smoothly on \(p\)."  
\end{remark}

\begin{remark}
    Note something very important: By the local trivialization condition, each vector space at each point isn't totally unrelated anymore (like we had with the tangent bundle); they are "bundled together" to locally look like the trivial bundle. So the requirement of the existence of the diffeomorphism \(\phi \) introduces will make it possible to relate vectors between different fibers locally, I think. 
\end{remark}

\begin{example}[Tangent Bundle]
    
\end{example}

\begin{definition}[Vector Bundle Isomorphism]
    Let \(E_1 \overset{\pi_1 }{\longrightarrow} M\) and \(E_2 \overset{\pi_2 }{\longrightarrow} M\) be vector bundles. An \textbf{isomorphism} is a diffeomorphism \(\phi : E_1 \to E_2\) such that for each \(p \in M\), \begin{align*}
        \phi \big|_{\pi ^{-1} (p)} : \pi _1 ^{-1} (p) \to \pi _2 ^{-1} (p)
    \end{align*} 
    is a vector space isomorphism.
    \[
    \begin{tikzcd}
    E_1 \arrow[dr, "\pi _1"'] \arrow[rr, "\phi", "\sim"'] & & E_2 \arrow[dl, "\pi _2"] \\
    & \mathcal{M}&
    \end{tikzcd}
    \]
\end{definition}

\begin{note}
    Contrast this to the local trivialization condition.
\end{note}
% anki-tex-guid: 728e35a86472

\begin{definition}{Trivial Bundle}
    A vector bundle \(E \overset{\pi }{\longrightarrow} \mathcal{M} \) is \textbf{trivial} if there is a vector bundle isomorphism \(E \cong \mathcal{M}\times \mathbb{R} ^k\). 
    
    In other words, a vector bundle is trivial if is is vector bundle isomorphic to the trivial bundle.
\end{definition}

\textbf{Motivating question:} When is a vector bundle trivial?

\begin{note}
   Fill out the rest of your lecture 23/10 here!
\end{note}


\newpage\lecture{18}{Tue 28 Oct}{1-forms, Integration and Exactness}
\paragraph{Abstract} The main purpose of the lecture was to study 1-forms on smooth manifolds and their relation to integration. We first defined 1-forms and what it means for them to be integrated along a path \(\gamma : [a, b] \to M\). We showed reparameterization invariance of this definition. Then we defined what it means for a 1-form to be \textbf{exact} and showed an important consequence of exactness: integration along curves only depends on endpoints.

However, if we want to check for exactness, and we can't infer a function \(f : \mathcal{M}\to \mathbb{R} \) such that \(\alpha  = df\), then it isn't feasible to integrate along all curves in \(M\) to check if the answer only depends on the endpoints. 

It turns out that in \(\mathbb{R} ^n\), one can show that exactness is equivalent to the statement that \begin{align*}
    \frac{\partial \alpha _i}{\partial x^j} = \frac{\partial \alpha _j}{\partial x^i}  
\end{align*} 
% anki-tex-guid: 157690ffdc38
for all \(i, j = 1, \dots , n\). We ended with an example of how this can fail when we aren't in \(\mathbb{R} ^n\) by explicit construction of a 1-form that gives back \(2 \pi \) times the winding number of a loop, which is always an integer. This is not trivial to show and will come back when we get to De Rham cohomology. 

\newpage\begin{definition}[1-form]
    Let \(M\) be a smooth manifold. A \textbf{1-form} \(\alpha\) on \(M\) is a smooth section of \(T^{\ast} M\). So for each \(p \in M\), \begin{align*}
        \alpha (p) : T_p \mathcal{M}\to \mathbb{R} 
    \end{align*}  
    is a linear map depending smoothly on \(p\). That is, in local coordinates \(x^1, \dots , x^n\) we can write \begin{align*}
        \alpha = \alpha _i dx^i, \qquad dx^i \left( \frac{\partial}{\partial x^j}  \right) = \delta ^i _j
    \end{align*}
    with \(\alpha _i (x^1, \dots , x^n) \in \mathbb{R} \) a smooth function of \(x^1, \dots , x^n\). \textred{When picking out coordinates with the differentials of the coordinate bases, should we evaluate at a point, or how does one formalize this with global differential. Is it just notation for that?} 
\end{definition}

Once we have 1-forms, we can define what it means to integrate them 
\begin{definition}[Integral of a 1-form]
    Let \(\gamma : [a, b] \to M\) be a smooth curve. The we define \begin{align*}
        \int _\gamma  \alpha = \int _a ^b \alpha (\gamma ^{\prime} (t))dt.
    \end{align*} 

    \textred{Input sketch here!}
\end{definition}

The definition above seems a bit circular: isn't \(dt\) a differential too, or what is going on here? For now we will accept Riemann/Lebesgue integration as being an operator giving us back the number we are used to when integrating in \(\mathbb{R} ^n\). We will come back to this.

% anki-tex-guid: aba0d76f4979
\begin{proposition}[Reparameterization Invariance]
     \begin{align*}
        \int _\gamma \alpha 
     \end{align*}
     is independent of the (oriented) parameterisation of \(\gamma \). 
\end{proposition}
\begin{proof}
    Let \(\varphi : [c, d] \to [a, b]\) be a diffeomorphism with \(c < d\). Then \textred{Fill out!}
\end{proof}

\begin{definition}[0-forms and the Global Differential]
    A \textbf{0-form} on \(M\) is a smooth function \(f: \mathcal{M}\to \mathbb{R} \). Define the \(\mathbb{R}\)-linear operator \begin{align*}
        d: \left\{ \text{0-forms} \right\} \to \left\{ \text{1-forms} \right\} 
    \end{align*} by it's action on \(v \in T_p M\)  \begin{align*}
        df(p)(v) = vf.
    \end{align*}
    In local coordinates \(x^1, \dots , x^n\) we have \begin{align*}
        df = \frac{\partial f}{\partial x^i} dx^i.
    \end{align*} 
\end{definition}

This is \textbf{exactly} what Lee calls the "global differential", and hence we've already seen this before and its properties. We will later call it the exterior derivative. Note that the global differential satisfies the Leibniz property. \textred{show this!}
% anki-tex-guid: 8991e48a5a34

\subsubsection{Integration and Exactness}
\begin{theorem}[Fundamental Theorem of Line Integrals]
    If \(f: \mathcal{M}\to \mathbb{R} \) is a smooth map and \(\gamma : [a, b] \to  M\) a smooth curve, then \begin{align*}
        \int _\gamma df = f(\gamma (b)) - f(\gamma (a))
    \end{align*}  
\end{theorem}
\begin{proof}
    \textred{Do it!}
\end{proof}

\begin{definition}[Exact 1-form]
    Let \(\alpha\) be a 1-form on \(M\). We say that \(\alpha\) is \textbf{exact} if \(\alpha = df\) for some smooth function \(f: \mathcal{M}\to \mathbb{R}\).
\end{definition}

\begin{proposition}[Integrating Exact 1-forms]
    Let \(\alpha\) be a 1-form on \(M\). Then \(\alpha\) is exact if and only if \begin{align}\label{prop:integrating_exact_1form}
        \text{for } \gamma : [a, b] \to  M\, \text{, } \int _\gamma \alpha \text{ depends only on } \gamma (a) \text{ and } \gamma (b)
    \end{align}  
\end{proposition}
\begin{proof}
    \textred{Fill out!}
\end{proof}
% anki-tex-guid: da37d991815e

\begin{proposition}[Exactness in \(\mathbb{R} ^n\)]
    Let \(\alpha = \alpha _i dx^i\) be a 1-form on \(\mathbb{R} ^n\). Then \(\alpha \) is exact if and only if \begin{align*}
        \frac{\partial \alpha _i}{\partial x^j} = \frac{\partial \alpha _j}{\partial x^i}  
    \end{align*}
    for all \(i, j = 1, \dots , n\).   
\end{proposition}

\begin{remark}
    The above result does not generalize to manifolds other than \(\mathbb{R} ^n\)! This will be illustrated by the following example. As we will see later, there is a precise notion of to what extent this condition fails given the manifold we are working on. Measuring this failure will be very important for generalizing integration to manifolds, as far as I know. See notes by Terry Tao.
\end{remark}

\begin{example}
    Consider the 1-form \begin{align*}
        \alpha  = \frac{xdy - ydx}{x^{2} +y^{2} }
    \end{align*}
    on \(\mathbb{R} ^2 \setminus \left\{ 0 \right\} \). It satisfies the hypothesis of the partial derivatives, as can be checked, but \(\alpha \) is still not exact! 
    
    To see why, consider the path \(\gamma : [0, 2 \pi ] \to  \mathbb{R} ^2 \setminus \left\{ 0 \right\} \) given by \(\gamma (t) = (\cos t, \sin t)\). Then \begin{align*}
        \int _\gamma \alpha  &= \int _0 ^2\pi  \left( \frac{xdy - ydx}{x^{2} +y^{2} } \right) \left( -\sin t \frac{\partial}{\partial x} + \cos t \frac{\partial}{\partial y} \right) dt\\
        &= \int _0 ^{2 \pi } \left( \cos (t) dy - \sin (t) dx \right)\left( -\sin t \frac{\partial}{\partial x} + \cos t \frac{\partial}{\partial y} \right) dt \\
        &= \int _0 ^{2 \pi } \left( \cos ^{2} t + \sin ^{2} t \right) dt \\
        &= \int _0 ^{2 \pi } dt \\
        &= 2 \pi 
    \end{align*}
    \textred{second line in above manipulation: is it a vector field, or should there be some points we are evaluating at?}

    But according to the reparameterisation invariance of integrals of 1-forms, we could just as well have chosen a path \(\gamma_1 : [0, 4 \pi ]\) or even the trivial path \(\gamma _0 : [0, 0]\) - they all share the same starting an endpoints (and orientation) \textred{the trivial path is a bit questionable since \(0 < 0\) isn't true...}, but they would produce different answers. Hence \(\alpha\) isn't exact, as the number we get out depends also on the path \(\gamma\), not just \(\gamma (a = 0) = (1, 0)\) and \(\gamma (b = 2 \pi ) = (1, 0)\).

    Switching to polar coordinates, we locally find that \(\alpha  = d \theta \). As we'll see later, we can think of this 1-form as a sort of \(\alpha  = d \theta \) everywhere \textit{locally}, in the sense that we will see that for this \(\alpha\), \begin{align*}
        \int _\gamma \alpha = 2\pi\ \cdot  (\text{winding number of \(\gamma \) around 0})
    \end{align*} 
    where the winding number is always an integer!
\end{example}

As the last example hinted at, the failure to be exact even when the 1-form satisfies the partial derivative criteria is related to the topology of the manifold. Note however that if \(n > 2\), then all 1-forms on \(\mathbb{R} ^n \setminus \left\{ 0 \right\} \) satisfying the partial derivative criterion will also be exact. I assume this has something to do with the fundamental group and being able to pull a loop around the puncture at the origin in all but 2 dimensions (in 1D we cannot have a loop). This we will show by studying de Rham cohomology.

\subsubsection{Introduction to 2-forms}
\begin{definition}[2-form]
    A \textbf{2-form} on \(\mathcal{M} \) is a smooth section \(\beta \) of the vector bundle \((TM \otimes TM)^{\ast} = \operatorname{Hom}(TM \otimes TM, \mathbb{R} ) \), which is antisymmetric. That is, for each \(p \in \mathcal{M} \), \(\beta (p): T_{p} \mathcal{M} \times T_p \mathcal{M}  \to \mathbb{R} \) is a function that \begin{enumerate}
        \item is bilinear.
        \item is antisymmetric. 
        \item depends smoothly on \(p\).  
    \end{enumerate}

    The first condition means that it descends to the tensor product \(T_p \mathcal{M}  \otimes T_p \mathcal{M} \) due to the universal property of the tensor product. See last lecture.

    In local coordinates \(x^1, \dots , x^n\), if \(i \neq j\), we define \(dx^i dx^j\) to be the 2-form such that \begin{align*}
        dx^i dx^j \left( \frac{\partial}{\partial x^k}, \frac{\partial}{\partial x^l}\right) = \delta ^i_ k \delta ^j _l
    \end{align*}   
    \textred{Are these supposed to be evaluated at a point? Or are they these type of global tangent vectors that restrict to the basis for each points?}
\end{definition}

\begin{proposition}
    Locally, any 2-form can be written uniquely as \begin{align*}
        \sum_{i < j} \beta _{ij} dx^i dx^j 
    \end{align*}

    We'll prove this later in a more general setting when looking at \(k\)-forms.
\end{proposition}

\begin{selfnote}
    Write out the proofs from this lecture and make sure you understand when there is supposed to be a point attached to the bases vectors for the tangent spaces and when there shouldn't be. Spell out once and for all what we mean when we say "depends smoothly on \(p\)". Write an addendum on the universal property of the tensor product as introduced last time. Central theme: Any bilinear map descends to the tensor space. 
\end{selfnote}

\newpage\lecture{19}{Thu 30 Oct}{2-forms, closed 1-forms and the first de Rham cohomology}

\paragraph{Abstract} The lecture focused on introducing 2-forms and relating them to 1-forms via. the exterior derivative introduced last time. We looked at alternative descriptions of 2-forms (actions at a point vs. actions on vector fields).

We revisited the exterior derivative turning 0-forms into 1-forms and showed how it should act on 1-forms to turn them into 2-forms. We first did the definition of this action in local coordinates. Hence to make sure it was a well-defined concept, we showed that the definition was independent of the chart chosen. Subsequently we introduced the coordinate free definition of the exterior derivatives action on 1-forms and showed that they were the same. This required the characterization of a 2-form as acting on vector-field and being bilinear with respect to function multiplication of vector fields, i.e. \begin{align*}
    \beta (fV, gW) = fg \beta (V, W)
\end{align*}

Introducing the notion of a closed differential, we then saw how \begin{align*}
    d (d \alpha ) = 0 \iff \frac{\partial \alpha _i}{\partial x^j} = \frac{\partial \alpha _j}{\partial x^i}  
\end{align*}
Since by construction, if \(f\) is a 0-form in \(\mathbb{R} ^n\), then \(df = \alpha \) is an exact differential, we immediately have made it clear that in \(\mathbb{R} ^n\), an exact 1-form is closed (by the previous lecture). Defining \begin{align*}
    H^1_{\text{dR}} \left(\mathcal{M} \right) = \frac{\left\{ \text{ closed 1-forms on } \mathcal{M} \ \right\} }{\left\{ \text{ exact 1-forms on } \mathcal{M} \ \right\} }
\end{align*}

In other words, we showed that
\begin{align*}
    H^1_{\text{dR}} \left(\mathbb{R} ^n\right) = \left\{ 0 \right\} 
\end{align*}
\newpage
\subsubsection{2-forms and the Exterior Derivative}
\paragraph{Recall} that a 1-form on \(M \) is a smooth section of \(T^{\ast} M\) while a 0-form is a smooth function \(f: M \to \mathbb{R} \). Note that if nothing else is stated, everything is smooth actually. Last lecture we also introduced the operator \begin{align*}
    d : \left\{ \text{ 0-forms } \right\} \to \left\{ \text{ 1-forms } \right\}.
\end{align*}
Given a 0-form \(f \in C^\infty(M)\), the resulting 1-form at a point \(p \in M\) with \(v \in T_p M\) acts by \begin{align*}
    df(p) (v) = df_p (v) = v(f).
\end{align*}

We also saw that a basis for 1-forms is given by \(dx^i\), such that any global differential / exterior derivative can in local coordinates \(x^1, \dots , x^n\) be written as \begin{align*}
    df = \frac{\partial f}{\partial x^i} dx^{i}. 
\end{align*}

Recall also the definition of a 2-form. Note that in the definition of a 2-form, we need only specify its action on two arbitrary basis vectors since a 2-form is bilinear.

Any 1-form can be uniqueley written as \(\alpha _i dx^i\) where \(\alpha _i (x^1, \dots , x^n)\) is a smooth function.  

\begin{proposition}
    Locally, any 2-form can be uniqueley written as \begin{align*}
        \sum_{i < j} \beta _{ij} dx^i dx^j
    \end{align*}
    where \(\beta _{ij}(x^1, \dots , x^n)\) is a smooth function.
\end{proposition}
\begin{proof}
    This will be done later in a more general setting.
\end{proof}

\begin{definition}[Exterior Derivative on 1-forms]
    Given local coordinates \(x^1, \dots , x^n\) and a 1-form \(\alpha = \alpha _i dx^i\) we define \begin{align*}
        d: \left\{ \text{ 1-forms } \right\} \to \left\{ \text{ 2-forms } \right\} 
    \end{align*}
    by its action \begin{align*}
        d \alpha = d(\alpha _i dx^i) = \frac{\partial \alpha _i}{\partial x^j} dx^j dx^i 
    \end{align*}
    Note that the term with \(i = j\) vanishes due to antisymmetry of the 2-form basis.  
\end{definition}

At this point, one may wonder why we even introduced antisymmetry in the first place. The reason for this will be clear later when we will introduce integration on manifolds, in which case orientations matter a whole lot. And these orientations will be "tracked" by \(k\)-forms, and their sign changes under index exchanges, I believe.

\begin{remark}
    Immediately one should be skeptical about the definition above, since it relies on local coordinates. This is one of the first definitions that we've made \textit{in} local coordinates. Note that there is a big difference between making the definition in local coordinates, and checking one of its properties in local coordinates. The former requires that we check that the definition is actually independent of that arbitrary choice used in the definition.
\end{remark}
 
\textred{Fill out the coordinate independence check here.}

\subsubsection{\(d^2 = 0\) and Closed 1-forms}

\begin{proposition}\label{prop:ddf_equal_0}
    If \(f\) is a 0-form, then \(d(df) = 0\). 
\end{proposition}
\begin{proof}
    Using local coordinates \(x^1, \dots , x^n\) we can write \begin{align*}
        d (df) = d\left(\frac{\partial f}{\partial x^i} dx^i\right) = \frac{\partial ^{2} f}{\partial x^i \partial x^j} dx ^j dx ^i  
    \end{align*} 

    Since this is a contraction of a symmetric object with an antisymmetric object, it must be zero.

    \textred{Again, are these the "global" partial Euclidean derivatives? Or how should this be understood? In fact, Hutchings wrote out} \begin{align*}
        \textred{\frac{\partial }{\partial x^j} \left( \frac{\partial f}{\partial x^i}  \right)  dx ^i dx^j}
    \end{align*}
\end{proof}

Recall again the definition of exactness, i.e. that a 1-form \(\alpha\) is exact if there exists some 0-form \(f \in C^\infty(M)\) such that \(\alpha = df\). Here comes anoter important property of 1-forms. 

\begin{definition}[Closed 1-form]
    A 1-form \(\alpha\) is said to be \textbf{closed} if \(d \alpha = 0\). In local coordinates \(x^1, \dots , x^n\) we can write \(\alpha  = \alpha _i dx^i\), such that \begin{align*}
        d \alpha = \frac{\partial \alpha _i}{\partial x^j} dx^j dx^i .
    \end{align*}
    Since the \(i = j\) term cancel, we can by antisymmetry write \begin{align*}
        d \alpha = \sum_{j < i} \left[ \frac{\partial \alpha _i}{\partial x^j} - \frac{\partial \alpha _j}{\partial x^i}   \right]  dx ^j dx^i
    \end{align*} 
    This means that \begin{align*}
        \alpha \text{ is closed} \iff \text{in local coordinates } \frac{\partial \alpha _i}{\partial x^j} = \frac{\partial \alpha _j}{\partial x^i}  
    \end{align*}
\end{definition}

From Proposition \ref{prop:ddf_equal_0} we see that for a 1-form \(\alpha\) on \(M\) \begin{align*}
    \alpha \text{ exact} \implies \alpha \text{ closed}
\end{align*} 
In \(\mathbb{R} ^n\), these are equivalent. 

\begin{example}[Exact iff Closed in \(\mathbb{R} ^n\)]
    Last lecture we saw how in \(\mathbb{R} ^n\), \(\alpha\) being exact is equivalent to \begin{align*}
        \frac{\partial \alpha _i}{\partial x^j} = \frac{\partial \alpha _j}{\partial x^i} 
    \end{align*}
    Hence we can immediately conclude that \(\alpha\) is closed in \(\mathbb{R} ^n\) if and only if \(\alpha \) is exact.

    We also saw however that on \(\mathbb{R} ^2 \setminus 0\), the 1-form \begin{align*}
        \alpha = \frac{x dy - y dx}{x^2 + y^2}
    \end{align*}
    is closed, but \textit{not} exact.
\end{example}
% anki-tex-guid: af6a3edc395a

\subsubsection{The First de Rham Cohomology}
\begin{definition}[The 1st de Rham Cohomology]
    Define the \textbf{1st de Rham Cohomology} \begin{align*}
        H^1 _{\text{dR}} \left(M\right) = \frac{\left\{ \text{ closed 1-forms on } M \ \right\} }{\left\{ \text{ exact 1-forms on } M \ \right\} }
    \end{align*} 
    Both of the sets are real vector spaces (\textred{check this!}), so this is a quotient space of vector spaces over \(\mathbb{R} \). 
\end{definition}

\begin{example}
    We've already shown that \begin{align*}
        H^1 _{\text{dR}} \left(\mathbb{R} ^n\right) = \left\{ 0 \right\}, 
    \end{align*} i.e. that every closed 1-form is exact in \(\mathbb{R} ^n\). We've also shown how a 1st de Rham cohomology can be non-trivial, i.e. that at least \begin{align*}
        H^1 _{\text{dR}} \left(\mathbb{R}^2 \setminus 0\right) \neq \left\{ 0 \right\} 
    \end{align*}

    In fact, we'll see later that \(H^1 _{\text{dR}} \left(\mathbb{R}^2 \setminus 0\right) \cong \mathbb{R} \). We will also see that if \(M\) is compact, then \begin{align*}
        \dim H^1 _{\text{dR}} \left(M\right) < \infty
    \end{align*}
    and learn how to compute the de Rham cohomology of reasonably well-described manifolds later.
\end{example}
% anki-tex-guid: f549a3544c8b

\subsubsection{Coordinate Free Exterior Derivative}
There is a coordinate free definition of the the differential sending 1-forms to 2-forms. 
\begin{definition}[Coordinate Free Exterior Derivative on 1-forms]
    If \(\alpha\) is a 1-form and \(V\) and \(W\) are vector fields, then \begin{align*}
        d \alpha = V\alpha (W) - W\alpha (V) - \alpha \left( \left[ V, W \right]  \right) 
    \end{align*} 
    with the bracket being the Lie bracket. \textred{Make sure that you understand how the vector fields and \(\alpha \) interact above. Is \(V\) acting on \(\alpha (W)\), or should it be a product?}.
\end{definition}

This is in some ways a strange definition, but this is because we can also think of 2-forms as maps turning vector fields into functions. In fact, a \(\mathbb{R}\)-bilinear antisymmetric function \begin{align*}
    \beta : \left\{ \text{ Vector fields } \right\} \times \left\{ \text{ Vector fields } \right\} \to \left\{ \text{ functions } \right\} 
\end{align*} 
\textit{defines} a 2-form if \(\beta (fV, gW) = fg \beta (V, W)\). The general statement of this is an exercise in the next lecture, so we defer comments. This would show that evaluating \(\beta \) at a point only depends on the vector fields at that point, i.e. on some vectors in \(T_p M \times T_p M\). \textred{which means what exactly?}. So let us check that above definition indeed has this property that makes it into a 2-form. By antisymmetry, it is enough to check the first slot: \begin{align*}
    \beta (f V, W) = \textred{fill out}
\end{align*} 

Now that we know that we indeed have a 2-form, we should check that the new coordinate free definition agrees with the first definition given in coordinates: 
\textred{Check this, very good exercise!}




\newpage\lecture{20}{Tue 04 Nov}{The Wedge Product, \(k\)-forms and the Exterior Derivative}[Lee \textred{??}]
\paragraph{Abstract} In the coming lectures, we want to generalize the notion of 0-, 1- and 2-forms which we have been working on the last few lectures. Specifically, we'll introduce certain operations on \(k\)-forms: \begin{itemize}
    \item Wedge product, \(\wedge\) 
    \item Exterior derivative, \(d\) 
    \item Pullback, \(\phi ^{\ast} \)
    \item Interior product, \(i_X\)
    \item Lie derivative, \(\mathcal{L} _X\)   
\end{itemize}In this lecture, we first introduce the space \(\operatorname{Alt}^k (V)\) and the notion of a wedge product between elements of different alternating spaces.

We use the wedge product to prove that \(\operatorname{Alt}^k(V)\) is a vector space of dimension \(\begin{pmatrix}
    n\\
    k
\end{pmatrix}\) by finding a basis for the space. We then see that given a vector space \(V\) of dimension \(n\), the space \(\operatorname{Alt}^n(V)\) relates to the determinant. 

Then we finally defined a \(k\)-form, and we see that it is indeed a generalization of what has been done in the last few lectures. We also prove an equivalent definition of a \(k\)-form. Introducing the notation \begin{align*}
    \Omega ^k (M) = \left\{ k\text{-forms on }M \right\} 
\end{align*}
we see that we should understand the wedge product as a map defined pointwise between the spaces \begin{align*}
    \wedge : \Omega ^k (M) \otimes \Omega ^l (M) \to \Omega ^{k + l} (M)
\end{align*}

We then introduced the exterior derivative and saw that it generalizes the specific \(d\) operators we've introduced on 0- and 1-forms already. We define this in local coordinates, so we must check that it is coordinate independent. Just like we've seen, this can be done by considering a change of charts and reproducing the same expression. But there is a more general way to see this by proving existence and uniqueness of a map satisfying certain properties. This is exactly the properties of the exterior derivative, and it matches the properties of our local coordinate definition (in \(\mathbb{R} ^n\)). Hence we can then conclude that they are they same.  

To extend beyond \(\mathbb{R} ^n\) one can use bump functions. But one can also write down a coordinate-free definition of the exterior derivative, which makes the coordinate independence manifest. 

Finally, we look at what the exterior derivative does in \(\mathbb{R} ^3\) and relates each \(\Omega^k (\mathbb{R} ^3)\) for \(k = 1, 2, 3\) to the gradient, curl and divergence. 
% anki-tex-guid: 962457c61218
\newpage

\subsubsection{The Alternating Subspace and the Wedge Product}

\begin{definition}[Alternating Subspace]
    Let \(V\) be a finite dimensional vector space and \(k\) a positive integer. Define the \textbf{alternating subspace} 
    \begin{align*}
        \operatorname{Alt}^k(V) = \left\{ \text{ Alternating multilinear maps } \alpha : \bigotimes _k V \to \mathbb{R}  \right\} 
    \end{align*}
    Note that once we specify that the map is multilinear, we can just use a Cartesian product (since every multilinear map factors through the tensor space). Or we can omit the word multilinear and specify that the map goes out of the tensor space. These are equivalent and doing both is a bit redundant. But we leave it here to emphasize this lesson. 

    In other terms, each element \(\alpha \in \operatorname{Alt}^k(V) \) is a function \(\alpha : V^k \to \mathbb{R} \) such that \begin{align*}
        &\alpha (c v_1, \dots , v_k) = c \alpha (v_1, \dots , v_k)\\
        &\alpha (v_1 + v_1^{\prime} , \dots , v_k) = \alpha (v_1, \dots , v_k) + \alpha (v_1^{\prime} , \dots , v_k)
    \end{align*} 
    and similarly for each entry. Again, multilinearity of \(\alpha\) means that the map factors through the tensor space. 
    
    The alternating property specifically means that if \(\sigma \in S_k \) is a permutation, then \begin{align*}
        \alpha (v_{\sigma (1)}, \dots , v_{\sigma (k)}) = (-1)^{\epsilon (\sigma )}\alpha (v_1, \dots , v_k)
    \end{align*}

    \(\operatorname{Alt}^k(V)\) is a vector space of dimension \(\begin{pmatrix}
    n\\
    k
\end{pmatrix}\), as we'll see in a second. Furthermore, it is a subspace of all multilinear maps out of \(\bigoplus _k V\). \textred{Prove this final claim at some point! It really just means proving that the bigger space is a vector space and showing that concluding that there exists multilinear maps which aren't alternating}.  
\end{definition}

\begin{definition}[Wedge Product]
    We define the \textbf{wedge product} \(\wedge: \operatorname{Alt}^k(V) \otimes \operatorname{Alt}^l(V) \to \operatorname{Alt}^{k + l}(V) \) by its action on \(\alpha \in \operatorname{Alt}^k(V)\) and \(\beta \in \operatorname{Alt}^l(V) \): \begin{align*}
        \alpha \wedge \beta (v_1, \dots , v_{k + l}) = \frac{1}{k! l!} \sum_{\sigma \in S_{k + l}} (-1)^{\epsilon (\sigma )} \alpha (v_{\sigma (1)}, \dots , v_{\sigma (k)}) \beta (v_{\sigma (k + 1)}, \dots , v_{\sigma (k + l)}) 
    \end{align*}   
\end{definition}

\begin{intuition}
    Some intuition for the combinatorial prefactor is that every time we have chosen \(l\) specific vectors to put in \(\beta \), there are still \(k!\) permutations in \(S_k\) that only permute \(k\) vectors that are left to go into \(\alpha \). We want to consider all of these equivalent, i.e. no such permutation is more important than the others and it is only important which vectors go into \(\alpha\), not their order. We will also see that this results in some equations later having nice normalization. 
\end{intuition}
\begin{example}
    If \(k = l = 1\), we explicitly get \textred{Fill this out} 
\end{example}

\begin{proposition}[Associativity of the wedge product]
    The wedge product is associative and \begin{align*}
        \beta \wedge \alpha = (-1)^{kl} \alpha \wedge \beta 
    \end{align*}
\end{proposition}
\begin{proof}
    Consider \(\alpha \in \operatorname{Alt}^k(V), \beta \in \operatorname{Alt}^l(V)\) and \(\eta \in \operatorname{Alt}^m(V)\). Then \begin{align*}
        \beta \wedge \eta (v_1, \dots , v_{l + m}) = \frac{1}{l! m!}\sum_{\sigma \in S_{l + m}} (-1)^{\epsilon (\sigma )} \beta (v_{\sigma (1)}, \dots , v_{\sigma (l)}) \eta (v_{\sigma (l + 1)}, \dots , v_{\sigma (l + m)}) 
    \end{align*}  
    such that \begin{align*}
        \alpha \wedge (\beta &\wedge \eta ) (v_1, \dots , v_{k + l + m}) \\
        &=\frac{1}{k ! (l + m)!} \sum_{\rho \in S_{k + l + m}}(-1)^{\epsilon (\rho )} \alpha (v_{\rho (1)}, \dots , v_{\rho (k)}) (\beta \wedge \eta )(v_{\rho (k + 1)}, \dots , v_{\rho (k + l + m)})\\
        &= \frac{1}{k ! (l + m)!} \frac{1}{l! m!} \sum_{\rho \in S_{k + l + m}} \sum_{\sigma \in S_{k + l}} (-1)^{\epsilon(\rho  )}(-1)^{\epsilon (\sigma )}  \alpha (v_{\rho (1)}, \dots , v_{\rho (k)}) \\
        &\hspace{8em} \cdot \beta (v_{\sigma(\rho (k + 1))}, \dots , v _ {\sigma(\rho (k + l))} ) \eta  (v_{\sigma (\rho(k + l + 1))}, \dots , v_{\sigma (\rho(k + l + m))})
    \end{align*}
    whereas \begin{align*}
        \alpha \wedge \beta (v_1, \dots , v_{k + l}) = \frac{1}{k! l!}\sum_{\sigma \in S_{k + l}} (-1)^{\epsilon (\sigma )} \alpha (v_{\sigma (1)}, \dots , v_{\sigma (k)}) \beta (v_{\sigma (k + 1)}, \dots , v_{\sigma (k + l)})
    \end{align*}
    such that \begin{align*}
        (\alpha \wedge \beta ) &\wedge \eta (v_1, \dots , v_{k + l + m}) 
        \\ &= \frac{1}{(k + l)! m!} \sum_{\rho \in S_{k + l + m}} (-1)^{\epsilon (\rho )}  (\alpha \wedge \beta ) (v_{\rho (1)}, \dots , v_{\rho (k + l)}) \eta (v_{\rho (k + l + 1)}, \dots , v_{\rho (k + l + m)} )\\
        &= \frac{1}{(k + l)! m!} \frac{1}{k! l!} \sum_{\rho \in S_{k + l + m}} \sum_{\sigma  \in S_{k + l}}  (-1)^{\epsilon (\rho )} (-1)^{\epsilon (\sigma )} \alpha (v_{\sigma (\rho (1))}, \dots , v_{\sigma (\rho (k))}) \\
        &\hspace{8em} \cdot \beta (v_{\sigma (\rho (k + 1))}, \dots , v_{\sigma (\rho (k + l))})\eta (v_{\rho (k + l + 1)}, \dots , v_{\rho (k + l + m)} )
    \end{align*}
% anki-tex-guid: fa074e944ac6
    \textred{There seems to be some problems here. Finish this at some point!}
\end{proof}

Let \(e_1, \dots , e_n\) be a basis for \(V\) and let \(e^1, \dots , e^n\) be the dual basis, which is defined by \(e^i (e_j) = \delta ^i _j\). \textred{Would be cool to explain why this is well-defined and exists!}

\begin{proposition}
    A basis for \(\operatorname{Alt}^k(V) \) is given by \begin{align*}
        e^{i_1} \wedge \dots \wedge e^{i _k}, \qquad 1 \leq i_1 < i_2 < \dots < i_k \leq n
    \end{align*} 
\end{proposition}

\begin{proof}
    Fill this out and truly understand it!
\end{proof}

\begin{example}[The Determinant]
    For an \(n\)-dimensional vector space \(V\), the space of "top forms" \begin{align*}
        \operatorname{Alt}^n(V) = \mathbb{R} \cdot e^1 \wedge \cdots \wedge e^n
    \end{align*} 
    is 1-dimensional. Every element takes in \(n\)vectors \(v_1, \dots , v_n\), and writing \(v_i = V_i ^j e_j\) the \(n\)-form acting on those vectors will be (proportional to) \(\det (V^j _ i)\).
    
    \textred{Prove why in local coordinates!}
\end{example}

\subsubsection{\(k\)-forms}

But what does all this have to do with manifolds, one might ask? 
\begin{definition}[\(k\)-forms]
    Let \(M\) be an \(n\)-dimensional smooth manifold. We then form the vector bundle \(\operatorname{Alt}^k(TM) \overset{\pi }{\longrightarrow} M\). (This is a subbundle of \((TM \otimes \cdots \otimes TM)^{\ast}\) \textred{prove why}). 
    
    The fiber over \(p \in M\) is \(\operatorname{Alt}^k(T_p M) \). A \textbf{\(k\)-form} on \(M\) is a section of this bundle. 

    This definition is indeed the generalization of what we've done so far. In local coordinates \(x^1, \dots , x^n\) a \(k\)-form can be written as \begin{align*}
        \alpha = \sum_{i_1 < \cdots < i_k} \alpha _{i_1 \cdots i_k} dx^{i_1} \wedge \cdots \wedge dx^{i_k},
    \end{align*} 
    where the \(\alpha _{i_1 \cdots i_k}\)'s are smooth functions of \(x^1, \dots , x^n\). When wedging by basis vectors, one often omits the wedges. This is exactly the notation we were actually using the last two lectures when looking only at 1- and 2-forms.

    We write \begin{align*}
        \Omega ^k (M) = \left\{\ k\text{-forms on M } \right\} 
    \end{align*}
\end{definition}

\begin{claim}
    An equivalent definition is that a \(k\)-form is an alternating, multilinear function \begin{align*}
        \operatorname{Vect}(M) \otimes \cdots \otimes \operatorname{Vect}(M) \to  C^\infty (M, \mathbb{R})  
    \end{align*}
    \textred{Exercise: Prove it!}
\end{claim}

% anki-tex-guid: 6367c999bfb4
With the new notation for the space of all \(k\)-forms, we see that we can understand the wedge product as a map \begin{align*}
    \wedge : \Omega ^k (M) \otimes \Omega ^l (M) \to \Omega ^{k + l} (M),
\end{align*} 
which is defined pointwise, i.e. we need to specify a point on \(M\) before we know what coordinates for our \(k\)-form that our section outputs \textred{I think?}.

\subsubsection{The Exterior Derivative}
\begin{definition}[Exterior Derivative in Local Coordinates]
    The \textbf{exterior derivative} is a map \(d : \Omega ^k (M) \to \Omega ^{k + 1} (M)\). Let \(I\) denote a \(k\)-tuple \((i_1, \dots , i_k)\) with \(1 \leq i_1 < \cdots < i_k \leq n\). Write \(\alpha = \alpha _I dx^I\). We then define \begin{align*}
        d \alpha = \frac{\partial \alpha _I}{\partial x^i} dx^i \wedge dx^I
    \end{align*} 
\end{definition}

\begin{example}
    Fill out \(k = 0\) and \(k = 1\). These exactly agree with our previous definitions from previous lectures. 
\end{example}

In general, \begin{align*}
    d(\alpha _I dx^I) = d \alpha _I \wedge dx^I
\end{align*}
\textred{Show this!}

Again, with a definition given in local coordinates, we have to be careful. Like last time, one can do an explicit computation to show that a change of coordinates leaves the form invariant. But there is a more sleek approach. 

% anki-tex-guid: 19ee3acd149d
\begin{lemma}\label{lemma:unique_exterior_deriv}
    There is a unique \(\mathbb{R} \)-linear map \(d: \Omega ^k (M) \to \Omega ^{k + 1} (M)\) such that \begin{enumerate}
        \item \(k = 0 \implies \left( df(p) \right)(v) = vf \) for \(v \in T_p M\).
        \item Obeys a Leibniz rule: If \(\alpha \in \Omega ^k\) and \(\beta \in \Omega ^l\), then \begin{align*}
            \boxed{d (\alpha \wedge \beta ) = d \alpha \wedge \beta + (-1)^k \alpha \wedge d \beta }
        \end{align*}
        \item \(d \circ d = 0\). 
    \end{enumerate}  
    Note that the second condition is a kind of differentiation, but that we have put a sign in there to seem mystical and cool. 
\end{lemma}
\begin{proof}
    \textred{Fill it out! Also comment about extension to general manifolds, since the proof is only for \(\mathbb{R} ^n\)}.
\end{proof}

\begin{definition}[The Exterior Derivative without Coordinates]
    If \(\alpha \) is a \(k\)-form and \(V_1, \dots , V_{k +1}\) are vector fields on \(M\), then \begin{align*}
        d \alpha &= \sum_{i} (-1)^{i - 1} V_i \left( \alpha \left( V_1, \dots , \hat{V}_i , \dots , V_{k + 1} \right)  \right) \\
        &\quad + \sum_{i < j} (-1)^{i + j} \alpha \left( [V_i, V_j], V_1, \dots , \hat{V}_i, \dots , \hat{V}_j, \dots , V_{k + 1} \right) 
    \end{align*}  
\end{definition}

One can check that this satisfies all the properties listed in the Lemma \ref{lemma:unique_exterior_deriv}, from which we can conclude that the definitions must be the same map then. And since this definition is coordinate free, the map itself is manifestly coordinate free too. 
% anki-tex-guid: 2420b66ef2f0

\begin{example}[\(\mathbb{R} ^3\) and vector calculus]
    \textred{Put in the nice diagram and the claims made! I.e. that we have gradient, curl and divergence. Put this together with our exact and closed differential, showing that curl(grad) = 0 and div(curl) = 0. Note however that div(grad) = laplacian - but our results say nothing about such compositions yet I believe.}
\end{example}

\newpage\lecture{21}{Thu 06 Nov}{Pullback, Integration on Manifolds and Stoke's Theorem Introduction}
\paragraph{Abstract} We started of introducing the notion of a \textbf{pullback of a \(k\)-form} and studied some nice properties: Contravariant functor and naturality. We discussed the notation used for \(k\)-forms in Lee and how it differs a bit from our notation. 

Then we defined integration of \(n\)-forms on \(n\)-dimensional smooth manifolds in multiple steps. We started in \(\mathbb{R} ^n\) and proved a small lemma: the pullback of an \(n\)-form by an orientation-preserving diffeomorphism behaves nicely with our definition of integration. This is exactly a \textbf{generalisation of ordinary change-of-variables} in multivariable calculus. 

After this, we define the \textbf{generalization of integration} to arbitrary manifolds via. partitions of unity. The proof of well-definedness is left out, since it is rather technical and uses something called a common refinement together with the naturality of the pullback. 

We made our definition concrete by considering integration on \(S^1\) and saw how it reproduces the ordinary Riemann integral we would expect. 

Finally, we stated \textbf{Stoke's Theorem} and outline the structure of the proof, which was left as an exercise to the reader. We will walk through it next lecture. 

\end{document}

