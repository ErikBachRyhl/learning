\documentclass[11pt]{article}
\input{../preamble.tex}   % path is relative to the topic folder

\title{Introduction to Partial Differential Equations}
\author{Erik Bach Ryhl}
\date{\today}

\graphicspath{{./figures}}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Borthwick Chapter 7 - Function Spaces Basics}

What does it mean for a sequence of functions \(f_n\) to converge to some function \(f\)? To even answer this question, we of course need to specify some norm within we consider this convergence with respect to. Hence there will always be a norm implied when working with convergence (this is true regardless of us considering functions or regular sequences). 

Each function could for example be a partial sum, so this also covers convergence of series when \(n \to \infty\).

\subsection{Hilbert Spaces and Banach Spaces}
\begin{definition}
    A \textbf{norm} on a vector space \(V\) is a function \(|| \cdot ||: V \to \mathbb{R} ^{\geq 0}\) satisfying \begin{itemize}
        \item \(||x|| = 0 \iff x = 0\)
        \item \(||cx|| = c ||x||\) for any constant \(c\) in the field of \(V\)
        \item \(||x + y|| \leq ||x|| + ||y||\)       
    \end{itemize}
\end{definition}

We can now consider whether we can give the same vector space \(V\) different norms. A common family of norms are the \(p\)-norms: \begin{align*}
    ||x||_p = \left( |x_1|^p + \dots + |x_n| ^p \right)^{1 / p}, \qquad p \geq 1
\end{align*}  
where the condition on \(p\) is necessary for the triangle inequality to not fail (we need our function to be convex, I think is the property). Hence for example \((\mathbb{R} ^n, || \cdot ||_p)\) is a whole family of vector spaces with the same underlying set, but with different notions of norm. 

Any vector space with a norm has a vector space algebra and topology and geometry induced by the norm. \textred{Understand this better!}. We therefore expect that different norms will induce different topologies and geometries. As it turns out, for \textit{finite} dimensional vector spaces, we have the inequality \begin{align*}
    ||x||_p \leq 2 ||x||_2
\end{align*}
which has the consequence that the topologies induced are in fact equivalent. \textred{Get back to showing these things one day!}

But in infinite dimensions, this is \textbf{not} the case.  

\begin{definition}
    A \textbf{Banach space} is a complete, normed vector space, \((V, || \cdot ||)\).  
\end{definition}

\begin{example}
    \(C^0, C^1, \dots , C^k\) are all Banach spaces for finite \(k\). Remember that \(||f||_{C^0} = \operatorname{max} _{x \in \Omega } |f(x)| \), \(||f||_{C^1} = \operatorname{max} _{x \in \Omega } |f(x)| + \operatorname{max} _{x \in \Omega } |\frac{df}{dx}(x)|\) etc.  \textred{fill out these norms!}. 

    But a very frustrating fact is that \(C^\infty\) is \textit{not} a Banach space.  
\end{example}

\begin{definition}
    An \textbf{inner product} is a bilinear function \begin{align*}
        \left\langle \cdot , \cdot \right\rangle : V \times V \to \mathbb{R},
    \end{align*} 
that obeys \begin{enumerate}
    \item \( \left\langle v , v \right\rangle \geq 0\) is a norm (the \textbf{induced norm})
    \item it is bilinear 
\end{enumerate}
If \(V\) is a complex vector space, then the inner products maps into \(\mathbb{C} \) and \(\left\langle  v,\alpha w \right\rangle = \overline{\alpha}  \left\langle v, w \right\rangle \) (yes, the mathematician's convention is opposite the physicists.)
\end{definition}

\begin{lemma}[Cauchy-Schwartz]
    \(\forall v, w \in V\), \begin{align*}
        \left| \left\langle v, w \right\rangle  \right| \leq ||v|| \cdot ||w||.
    \end{align*} 
\end{lemma}
\begin{proof}
    It is an algebraic trick to prove it in the finite dimensional case. But I do not know how the proof goes in the infinite dimensional case. \textred{This would be cool to learn at one point!}
\end{proof}

\newpage\section{Numerical Techniques}
\textbf{Key concepts}
\begin{itemize}
    \item Discretisation map \(\mathcal{D} : \mathbb{R} ^N \to  L^2 (\Omega; \mathbb{R} )\) 
    \item Projection map 
\end{itemize} 

\newpage\section{Integral Solutions, Green's Functions and Fourier Transforms}

\textbf{Abstract:} Where are we going with this? What do we know now? What is the main motivating question for the next section? What are questions we are still totally unable to answer? 

\newpage\lecture{22}{Fri 24 Oct}{Maximum Principles I: Poisson's Formula}
\paragraph{Main Idea} We saw in lecture 9 how to solve (Poisson's equation?) in general on a (circular?) disk in general by doing a Fourier series of our boundary condition function which we could then plug in the coefficients for in the general solution. But this is tedious and takes much work to do every time.

So what if we just keep the function on the general abstract form an work with the Fourier coefficients as abstract entities? Can we rewrite the whole expression in a nice way? The answer is affirmative (under certain conditions like uniform convergence (met due to \(L^2\) arguments I think, check this out)) and on certain domains. 

To do this "trick" we will introduce harmonic functions, harmonic polynomials and see that going to complex variables shows that \(z^n\) generates harmonic polynomials in both real and imaginary parts. These in fact generate each "half" of a binomial expansion in the polynomial \((x + y)^n\), but just with some alternating signs due to \(z^n = (x + iy)^n\). Pretty cool stuff. Now, are these a basis for all harmonic polynomials? It will turn out that the answer is yes I think. Of course we have strong shades of \(z^n\) being a basis for all analytic functions, and each analytic function can be split into harmonic conjugate real and imaginary parts. So I don't think this should be too hard to prove with some methods from complex analysis. Maybe it is lowkey already proven, just not stated in that way. 

Before we prove Poisson, we introduce the idea of integral kernels and dummy variables. With this form, 

Anyway, we prove Poisson's formula as the first example of going directly to an integral solution of a Partial Differential Equaion. 

\newpage\subsubsection{Example Problems}
\begin{note}
    Solve these problems you missed: 
    
    \textbf{Olver 4.3.25 (d)}: \begin{align*}
        \Delta u = 0,\quad x^{2}+y^{2}<1,\qquad \dfrac{\partial u}{\partial \mathbf n} = x\ \text{on } x^{2}+y^{2}=1.
    \end{align*}

    \textbf{Olver 4.3.45}: Let \(p(x,y)\) be a polynomial (not necessarily harmonic). Suppose \(u(x,y)\) is harmonic and equals \(p(x,y)\) on the unit circle \(x^{2} + y^{2} = 1\). Prove that \(u(x, y)\) is a harmonic polynomial. 
\end{note}

\newpage\lecture{24.5}{Wed 29 Oct}{Regularity for Poisson and Maximum Principle for Heat Equation}
\paragraph{Abstract} Last lecture we proved existence and uniqueness for Poisson's equation. In this lecture, we first look at the regularity of such solutions. The regularity question is concerned with relating the smoothness of \(f\) in the problem \begin{align*}
    \Delta u = f
\end{align*}
to the smoothness of the solution function \(u\). The \textbf{Elliptic Regularity} theorem says that if \(f \in \mathcal{C} ^\infty(\Omega : \mathbb{R} )\), then indeed \(u \in \mathcal{C} ^\infty (\operatorname{Int} \Omega : \mathbb{R}  )\). 

We introduced the notion of \textbf{analytic functions} and showed how they are a subset of smooth functions. We then strengthened the elliptic regularity theorem. This strenghtened version is just a special case of a the "non-isolated points of analytic functions"-theorem from complex analysis and the fact that analytic functions give rise to harmonics. 

We then proved the \textbf{Maximum Principle for the Heat Equation} in a simple case. From this, we obtain the minimum principle and use this to prove uniqueness to solutions of the heat equation. This allowed us to conclude that the solution we've already obtained through separation of variables is the only one. Existence and regularity is handled in a similar fasion as with Poisson's equation. We might return to this.

Finally, it was noted that there is no maximum principle to the wave equation; this makes intuitive sense, since a plane wave solution has no (global) maximum or minimum. However, it is still possible to prove uniqueness, existence and regularity, albeit it is more difficult. 

\paragraph{Questions} 
\begin{itemize}
    \item \(\mathcal{C} ^\infty (\Omega : \mathbb{R} )\) is not a Banach space. Greg said "there is no norm for it" - but I thought it was because it was incomplete (i.e. we can make smooth functions converge to a step-function etc). Which one is it?
\end{itemize}


\newpage\lecture{25}{Fri 31 Oct}{Distributions and the Dirac Delta Function}[Olver 6.1]

\paragraph{Abstract} Fill out

\paragraph{Questions} Fill out

\begin{example}[Dirac delta function]
    The Dirac delta function is sometimes written as \begin{align*}
        \delta (x) = \begin{dcases}
            \infty \quad &x = 0\\
            0 \quad &x \neq 0
        \end{dcases}
    \end{align*}
    while \begin{align*}
        \int _{- \infty} ^\infty \delta (x) dx = 1.
    \end{align*}

    This is clearly not a function, as \(\infty\) isn't a number even. So what do we actually mean when writing this, and how can we make such a concept well-defined? 
\end{example}  

\subsubsection{The Duality Definition}
From finite vector spaces, one can construct many new vector spaces, including \(\operatorname{Hom}(V, W)\), the direct sum \(V \oplus W\) and the tensor space \(V \otimes W\). 

If a finite dimensional vector space \(V\) is equipped with an inner product, we can canonically identify it with the space \(\operatorname{Hom}(V, \mathbb{R})\) through the pairing \begin{align*}
    v \mapsto \left\langle v, \cdot \right\rangle \in  \operatorname{Hom}(V, \mathbb{R})
\end{align*}
for \(v \in V\). We define \(V^{\ast} \coloneqq \operatorname{Hom}(V, \mathbb{R} ) \) as the dual space of \(V\).  

This canonical isomorphism doesn't hold in general for infinite-dimensional vector spaces, but we can still make a sensible definition of the dual space that generalizes to infinite-dimensional vector spaces. 

\begin{definition}[The Dual Space]
    Given any vector space \(V\), define the \textbf{dual space of \(V\)} as
    \begin{align*}
        V^{\ast} \coloneqq \Set{\xi: V \to \mathbb{R} }{\xi \text{ is a continuous linear map}}. 
    \end{align*} 
\end{definition}
\begin{remark}
    Note that for finite dimensional vector spaces, the continuity requirement is redundant, since every linear map between finite dimensional vector spaces is smooth. Hence in this case, \(V^{\ast}\) just collapses to \(\operatorname{Hom}(V, \mathbb{R} ) \), i.e. the space of linear maps from \(V\) to \(\mathbb{R} \).  
\end{remark}

\begin{proposition}
    \(V^{\ast} \) is a vector space. 
\end{proposition}

If we represent elements from \(V\) as column vectors, then elements of \(V^{\ast}\) can be represented by row vectors.

\begin{example}[Integration]
    \textred{Fill out}
\end{example}

\begin{example}[Dual of \(L^2(\Omega: \mathbb{R} )\)]
    Given some function \(g \in L^2 (\Omega : \mathbb{R} )\), we can define an element \(\xi _g \in L^2 (\Omega : \mathbb{R} )^{\ast} \) by its action on other \(f \in L^2(\Omega: \mathbb{R} )\) \begin{align*}
        \xi _g [f] \coloneqq \left\langle g, f \right\rangle _{L^2(\Omega: \mathbb{R} )} = \int _\Omega g f dV
    \end{align*}
\end{example}

This is exactly the same pairing that gave us a canonical isomorphism in the case with finite dimensional vector spaces. An immediate question follows: Are \(L^2\) and \(\left( L^2 \right)^{\ast}  \) equally "big"? 

\begin{proposition}[Duality Reverses Inclusion]
    Recall that \begin{align*}
        L^2(\Omega : \mathbb{R} ) \supseteq \mathcal{C}^0 (\Omega : \mathbb{R} ) \supseteq \mathcal{C} ^1 (\Omega : \mathbb{R} ) \supseteq \cdots \supseteq \mathcal{C} ^\infty(\Omega : \mathbb{R} ) 
    \end{align*}
    However, with dual spaces, it goes the other way. 
\end{proposition}

% Intuition

\newpage\lecture{26}{Mon 03 Nov}{Green's Functions}[Olver 6.2]
\paragraph{Abstract} We recalled the definition of the Diract delta function. Then, we took a step back and looked at the series solution to Poisson's equation in terms of the eigenfunctions. In other words, we revisited the \textbf{superposition principle} and rewrote our series solution in terms of an integral over a discrete dummy variable.  



\end{document}




